<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>WebXR Image Tracking + Video</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
    <style>
      body { margin: 0; overflow: hidden; background: #000; }
      video { display: none; } /* Hide HTML video element */
    </style>
  </head>
  <body>
    <!-- Hidden HTML video element -->
    <video id="arVideo" src="video.mp4" loop playsinline muted></video>

    <script type="module">
      import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.150.0/build/three.module.js';
      import { ARButton } from 'https://cdn.jsdelivr.net/npm/three@0.150.0/examples/jsm/webxr/ARButton.js';

      let camera, scene, renderer;
      let video, videoTexture, videoPlane;
      let trackedImage = null;
      let refSpace = null;

      init();
      animate();

      async function init() {
        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera();

        renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.xr.enabled = true;
        document.body.appendChild(renderer.domElement);

        // ✅ Load and convert the marker image to ImageBitmap
        const imgResponse = await fetch('marker.jpg');
        const imgBlob = await imgResponse.blob();
        const imageBitmap = await createImageBitmap(imgBlob);

        // ✅ Add ARButton with image tracking enabled
        const button = ARButton.createButton(renderer, {
          requiredFeatures: ['image-tracking'],
          trackedImages: [
            {
              image: imageBitmap,
              widthInMeters: 0.2 // adjust to your printed image width
            }
          ]
        });
        document.body.appendChild(button);

        // ✅ Add lighting
        const light = new THREE.HemisphereLight(0xffffff, 0xbbbbff, 1);
        scene.add(light);

        // ✅ Setup video + texture
        video = document.getElementById('arVideo');
        videoTexture = new THREE.VideoTexture(video);
        const material = new THREE.MeshBasicMaterial({ map: videoTexture });
        const geometry = new THREE.PlaneGeometry(0.2, 0.1125); // 16:9 aspect ratio
        videoPlane = new THREE.Mesh(geometry, material);
        videoPlane.visible = false;
        scene.add(videoPlane);

        // Handle XR session start
        renderer.xr.addEventListener('sessionstart', () => {
          const session = renderer.xr.getSession();
          session.requestReferenceSpace('local').then(space => refSpace = space);
        });

        window.addEventListener('resize', () => {
          renderer.setSize(window.innerWidth, window.innerHeight);
        });
      }

      function animate() {
        renderer.setAnimationLoop(render);
      }

      function render(timestamp, frame) {
        if (frame && refSpace) {
          const results = frame.getImageTrackingResults
            ? frame.getImageTrackingResults()
            : [];

          for (const result of results) {
            const state = result.trackingState;
            const pose = frame.getPose(result.imageSpace, refSpace);

            if (state === 'tracked' && pose) {
              videoPlane.visible = true;
              videoPlane.matrix.fromArray(pose.transform.matrix);
              videoPlane.matrixAutoUpdate = false;
              if (video.paused) video.play();
            } else if (state === 'emulated' || state === 'untracked') {
              videoPlane.visible = false;
              video.pause();
            }
          }
        }

        renderer.render(scene, camera);
      }
    </script>
  </body>
</html>
